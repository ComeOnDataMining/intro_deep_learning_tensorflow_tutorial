{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MNIST dataset\n",
    "The dataset contains 60,000 handwritten digits for training and 10,000 handwritten digits for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "<img src=\"./image/mnist.png\" alt=\"MNIST\" width=\"400\"/>\n",
    "\n",
    "More info [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a Tensorflow-based network\n",
    "## 2.1 Main file ```tutorial.py```\n",
    "- Import necessary libraries and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import network files\n",
    "from dnn_config import DNN_Config   # uncomment for DNN\n",
    "from dnn import DNN                 # uncomment for DNN\n",
    "#from cnn_config import CNN_Config  # uncomment for CNN\n",
    "#from cnn import CNN                # uncomment for CNN\n",
    "#from rnn_config import RNN_Config  # uncomment for RNN\n",
    "#from rnn import RNN                # uncomment for RNN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some miscellaneous parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Whether device soft placement is allowed\n",
    "tf.app.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# Whether device placements should be logged\n",
    "tf.app.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "# Where to keep some output files\n",
    "tf.app.flags.DEFINE_string(\"out_dir\", \"./output/\", \"Point to output directory\")\n",
    "# Where to keep network's checkpoints\n",
    "tf.app.flags.DEFINE_string(\"checkpoint_dir\", \"./checkpoint/\", \"Point to checkpoint directory\")\n",
    "\n",
    "# Print out the parameters\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()): # python3\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "# create path where some output and checkpoints are stored (if not existed)\n",
    "out_path = os.path.abspath(os.path.join(os.path.curdir,FLAGS.out_dir))\n",
    "checkpoint_path = os.path.abspath(os.path.join(out_path,FLAGS.checkpoint_dir))\n",
    "if not os.path.isdir(os.path.abspath(out_path)): os.makedirs(os.path.abspath(out_path))\n",
    "if not os.path.isdir(os.path.abspath(checkpoint_path)): os.makedirs(os.path.abspath(checkpoint_path))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an object for network configuration and load MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Creat a network configuration object\n",
    "config = DNN_Config()   # uncomment for DNN\n",
    "# config = CNN_Config() # uncomment for CNN\n",
    "# config = RNN_Config() # uncomment for RNN\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Trainging Parameters\n",
    "learning_rate = 1e-3        # learning rate\n",
    "num_training_step = 1000    # number of training steps\n",
    "batch_size = 128            # batch size\n",
    "display_every = 10          # how often to display training progress\n",
    "evaluate_every = 10         # how often to evaluate the trained model on test data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create a Tensorflow graph\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "                                  log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    # Start a Tensorflow session\n",
    "    with sess.as_default():\n",
    "\n",
    "        # Network construction\n",
    "        net = DNN(config=config)    # uncomment for DNN\n",
    "        # net = CNN(config=config)  # uncomment for CNN\n",
    "        # net = RNN(config=config)  # uncomment for RNN\n",
    "\n",
    "        # Define training procedure (i.e. optimization procedure)\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(net.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # the saver object is to save a trained model (i.e. a checkpoint) to files\n",
    "        saver = tf.train.Saver(tf.all_variables(), max_to_keep = 5)\n",
    "\n",
    "        # initialize all variables\n",
    "        print(\"Model initialized\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # one training step\n",
    "        def train(x_batch, y_batch):\n",
    "\n",
    "            # Can you guess why we need this line?\n",
    "            #seq_len = np.ones(len(x_batch),dtype=int) * config.timesteps\n",
    "\n",
    "            feed_dict = {\n",
    "                net.X: x_batch,\n",
    "                net.y: y_batch,\n",
    "                net.dropout_keep_prob: config.dropout_keep_prob,\n",
    "                #net.seq_len: seq_len # Can you guess why we need this line?\n",
    "            }\n",
    "            _, step, loss, acc = sess.run([train_op, global_step, net.loss, net.accuracy], feed_dict)\n",
    "            return step, loss, acc\n",
    "\n",
    "        # one evaluation step\n",
    "        def eval(x_batch, y_batch):\n",
    "\n",
    "            # Can you guess why we need this line?\n",
    "            #seq_len = np.ones(len(x_batch),dtype=int) * config.timesteps\n",
    "\n",
    "            feed_dict = {\n",
    "                net.X: x_batch,\n",
    "                net.y: y_batch,\n",
    "                net.dropout_keep_prob: 1.0,\n",
    "                #net.seq_len: seq_len  # Can you guess why we need this line?\n",
    "            }\n",
    "            _, loss, yhat, acc = sess.run(\n",
    "                [global_step, net.loss, net.y_hat, net.accuracy],\n",
    "                feed_dict)\n",
    "            return loss, acc, yhat\n",
    "\n",
    "        # Training procedure\n",
    "        for step in range(1, num_training_step + 1):\n",
    "            # generate a mini-batch of data for training\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Can you guess why we need this line?\n",
    "            # x_batch = np.reshape(x_batch, (-1, config.input_height, config.input_width, config.input_channel))\n",
    "\n",
    "            # Can you guess why we need this line?\n",
    "            # reshape to (batch_size, num_input)\n",
    "            # x_batch = np.reshape(x_batch, (-1, config.timesteps, config.num_input))\n",
    "\n",
    "            # train the network with the generated mini-batch of data\n",
    "            train_step, train_loss, train_acc = train(x_batch, y_batch)\n",
    "\n",
    "            # display training progress?\n",
    "            if step % display_every == 0:\n",
    "                #time_str = datetime.now().isoformat()\n",
    "                print(\"Step {}, loss {:.4f}, accuracy {:.3f}\".format(train_step, train_loss, train_acc))\n",
    "\n",
    "            # model evaluation?\n",
    "            if step % evaluate_every == 0:\n",
    "                test_X = mnist.test.images\n",
    "\n",
    "                # Can you guess why we need this line?\n",
    "                # test_X = np.reshape(test_X, (-1, config.input_height, config.input_width, config.input_channel))\n",
    "\n",
    "                # Can you guess why we need this line?\n",
    "                # reshape to (batch_size, num_input)\n",
    "                # test_X = np.reshape(test_X, (-1, config.timesteps, config.num_input))\n",
    "\n",
    "                test_y = mnist.test.labels\n",
    "                test_loss, test_acc, test_yhat = eval(test_X, test_y)\n",
    "                print(\"Evaluation: loss {:.4f}, accuracy {:.3f}\".format(test_loss, test_acc))\n",
    "\n",
    "                # save the current model (i.e. checkpoint) to files\n",
    "                checkpoint_name = os.path.join(checkpoint_path, 'model_step' + str(step))\n",
    "                save_path = saver.save(sess, checkpoint_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Neural Network (DNN)\n",
    "\n",
    "<img src=\"./image/dnn.jpeg\" alt=\"DNN\" width=\"750\"/>\n",
    "\n",
    "## 3.1 Network configuration ```DNN_Config.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __init__(self):\n",
    "\n",
    "    self.n_hidden_1 = 1024 # 1st layer number of neurons\n",
    "    self.n_hidden_2 = 1024 # 2nd layer number of neurons\n",
    "    self.num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "    self.num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    self.dropout_keep_prob = 0.9\n",
    "    self.l2_reg_lambda = 1e-4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DNN class ```DNN.py```\n",
    "\n",
    "- Input handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __init__(self, config):\n",
    "    self.config = config\n",
    "    # place holder for input feature vectors and one-hot encoding output\n",
    "    self.X = tf.placeholder(\"float\", shape=[None, self.config.num_input], name='X')\n",
    "    self.y = tf.placeholder(\"float\", shape=[None, self.config.num_classes], name='y')\n",
    "    # place holder for dropout\n",
    "    self.dropout_keep_prob = tf.placeholder(\"float\", name=\"dropout_keep_prob\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def construct(self):\n",
    "    # layers weight & bias\n",
    "    self.w = {\n",
    "        'h1': tf.Variable(tf.random_normal([self.config.num_input, self.config.n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([self.config.n_hidden_1, self.config.n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([self.config.n_hidden_2, self.config.num_classes]))\n",
    "    }\n",
    "    self.b = {\n",
    "        'h1': tf.Variable(tf.random_normal([self.config.n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([self.config.n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([self.config.num_classes]))\n",
    "    }\n",
    "\n",
    "    #with tf.device('/gpu:0'), tf.variable_scope(\"fully-connected-layers\"):\n",
    "    with tf.device('/cpu:0'), tf.variable_scope(\"fully-connected-layers\"):\n",
    "        h1 = tf.add(tf.matmul(self.X, self.w['h1']), self.b['h1'])\n",
    "        h1_relu = tf.nn.relu(h1)\n",
    "        h1_dropout = tf.nn.dropout(h1_relu, self.dropout_keep_prob)\n",
    "\n",
    "        h2 = tf.add(tf.matmul(h1_dropout, self.w['h2']), self.b['h2'])\n",
    "        h2_relu = tf.nn.relu(h2)\n",
    "        h2_dropout = tf.nn.dropout(h2_relu, self.dropout_keep_prob)\n",
    "\n",
    "    # network's output\n",
    "    with tf.device('/cpu:0'), tf.variable_scope(\"output\"):\n",
    "        self.output = tf.add(tf.matmul(h2_dropout, self.w['out']), self.b['out']) # logit\n",
    "        self.y_hat = tf.argmax(self.output, 1, name='y_hat') # predicted labels\n",
    "\n",
    "    # network's losses\n",
    "    with tf.device('/cpu:0'), tf.name_scope(\"loss\"):\n",
    "        # cross-entropy loss\n",
    "        self.output_loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.output)\n",
    "        self.output_loss = tf.reduce_mean(self.output_loss) # summing over all samples of the batch\n",
    "\n",
    "        # add on regularization\n",
    "        l2_loss = self.config.l2_reg_lambda * \\\n",
    "                  sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "\n",
    "        # total loss\n",
    "        self.loss = self.output_loss + l2_loss\n",
    "\n",
    "    # calculate accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(self.y_hat, tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Convolutional Neural Network (CNN)\n",
    "\n",
    "<img src=\"./image/cnn.png\" alt=\"GRU cell\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Network configuration ```cnn_config.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __init__(self):\n",
    "    self.input_height = 28 # MNIST data input (img shape: 28*28*1)\n",
    "    self.input_width = 28 # MNIST data input (img shape: 28*28*1)\n",
    "    self.input_channel = 1 # MNIST data input (img shape: 28*28*1)\n",
    "    self.num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    self.dropout_keep_prob = 0.9\n",
    "    self.l2_reg_lambda = 1e-4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 CNN class ```cnn.py```\n",
    "\n",
    "- Input handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __init__(self, config):\n",
    "    self.config = config\n",
    "    # place holder for input feature vectors and one-hot encoding output\n",
    "    self.X = tf.placeholder(\"float\",\n",
    "                            shape=[None,\n",
    "                                   self.config.input_height,\n",
    "                                   self.config.input_width,\n",
    "                                   self.config.input_channel],\n",
    "                            name='X')\n",
    "    self.y = tf.placeholder(\"float\", shape=[None, self.config.num_classes], name='y')\n",
    "    # place holder for dropout\n",
    "    self.dropout_keep_prob = tf.placeholder(\"float\", name=\"dropout_keep_prob\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(self, X, W, b, stride = 1): # conv2d wrapper\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    conv = tf.nn.conv2d(X, W, strides = [1, 1, 1, 1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, b)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    return conv\n",
    "\n",
    "# maxpool2d wrapper\n",
    "def maxpool2d(self, X, k = 2):\n",
    "    return tf.nn.max_pool(X, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def construct(self):\n",
    "    # layers weight & bias\n",
    "    self.w = {\n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "        # 5x5 conv, 32 inputs, 64 outputs\n",
    "        'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.random_normal([1024, self.config.num_classes]))\n",
    "    }\n",
    "\n",
    "    self.b = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32])),\n",
    "        'bc2': tf.Variable(tf.random_normal([64])),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "        'out': tf.Variable(tf.random_normal([self.config.num_classes]))\n",
    "    }\n",
    "\n",
    "    # Convolutional layers\n",
    "    with tf.device('/cpu:0'), tf.variable_scope(\"conv-layers\"):\n",
    "        # Conv layer 1\n",
    "        conv1 = self.conv2d(self.X, self.w['wc1'], self.b['bc1'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv1_pool = self.maxpool2d(conv1, k = 2)\n",
    "        conv1_dropout = tf.nn.dropout(conv1_pool, self.dropout_keep_prob)\n",
    "\n",
    "        # Conv Layer 2\n",
    "        conv2 = self.conv2d(conv1_dropout, self.w['wc2'], self.b['bc2'])\n",
    "        # Max Pooling (down-sampling)\n",
    "        conv2_pool = self.maxpool2d(conv2, k = 2)\n",
    "        conv2_dropout = tf.nn.dropout(conv2_pool, self.dropout_keep_prob)\n",
    "\n",
    "    # fully connected layer\n",
    "    with tf.device('/cpu:0'), tf.variable_scope(\"fully-connected-layers\"):\n",
    "        # flatten conv feature map\n",
    "        flattened = tf.reshape(conv2_dropout, [-1, self.w['wd1'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(flattened, self.w['wd1']), self.b['bd1'])\n",
    "        fc1_relu = tf.nn.relu(fc1)\n",
    "        fc1_dropout = tf.nn.dropout(fc1_relu, self.dropout_keep_prob)\n",
    "\n",
    "    # network's output\n",
    "    with tf.device('/cpu:0'), tf.variable_scope(\"output\"):\n",
    "        self.output = tf.add(tf.matmul(fc1_dropout, self.w['out']), self.b['out']) # logit\n",
    "        self.y_hat = tf.argmax(self.output, 1, name='y_hat') # predicted labels\n",
    "\n",
    "    # network's losses\n",
    "    with tf.device('/cpu:0'), tf.name_scope(\"loss\"):\n",
    "        # cross-entropy loss\n",
    "        self.output_loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.output)\n",
    "        self.output_loss = tf.reduce_mean(self.output_loss) # summing over all samples of the batch\n",
    "\n",
    "        # add on regularization\n",
    "        l2_loss = self.config.l2_reg_lambda * \\\n",
    "                  sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "\n",
    "        # total loss\n",
    "        self.loss = self.output_loss + l2_loss\n",
    "\n",
    "    # calculate accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(self.y_hat, tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Recurrent Neural Network (RNN)\n",
    "<img src=\"./image/rnn.png\" alt=\"GRU cell\" width=\"750\"/>\n",
    "\n",
    "\n",
    "#### Gated Recurrent Unit (GRU) cell\n",
    "<img src=\"./image/GRU.png\" alt=\"GRU cell\" width=\"350\"/>\n",
    "\n",
    "References:\n",
    "\n",
    "- Sepp Hochreiter & Jurgen Schmidhuber, _Long Short Term Memory,_ Neural Computation 9(8): 1735-1780, 1997.\n",
    "- K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio, _Learning phrase representations using RNN encoderdecoder for statistical machine translation,_ in Proc. EMNLP, 2014, pp. 1724–1734."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Network configuration ```rnn_config.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __init__(self):\n",
    "\n",
    "    self.n_hidden = 256 # 1st layer number of neurons\n",
    "    self.num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "    self.timesteps = 28\n",
    "    self.num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "    self.dropout_keep_prob = 0.9\n",
    "    self.l2_reg_lambda = 1e-4 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Network architecture ```rnn.py```\n",
    "- Input handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __init__(self, config):\n",
    "    self.config = config\n",
    "    # place holder for input feature vectors and one-hot encoding output\n",
    "    self.X = tf.placeholder(\"float\",\n",
    "                            shape=[None, self.config.timesteps, self.config.num_input],\n",
    "                            name='X')\n",
    "    self.y = tf.placeholder(\"float\", shape=[None, self.config.num_classes], name='y')\n",
    "    # place holder for dropout\n",
    "    self.dropout_keep_prob = tf.placeholder(\"float\", name=\"dropout_keep_prob\")\n",
    "\n",
    "    self.seq_len = tf.placeholder(tf.int32, [None]) # for the dynamic RNN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def construct(self):\n",
    "    # Define weights\n",
    "    self.w = {\n",
    "        'out': tf.Variable(tf.random_normal([self.config.n_hidden, self.config.num_classes]))\n",
    "    }\n",
    "    self.b = {\n",
    "        'out': tf.Variable(tf.random_normal([self.config.num_classes]))\n",
    "    }\n",
    "\n",
    "    with tf.device('/cpu:0'), tf.name_scope(\"recurrent_layer\"):\n",
    "        # Define a lstm cell with tensorflow\n",
    "        #rnn_cell = tf.contrib.rnn.BasicLSTMCell(self.config.n_hidden, forget_bias=1.0)\n",
    "        rnn_cell = tf.contrib.rnn.GRUCell(self.config.n_hidden)\n",
    "        # Get RNN cell output\n",
    "        #outputs, states = tf.contrib.rnn.static_rnn(rnn_cell, self.X, dtype=tf.float32)\n",
    "        outputs, states = tf.nn.dynamic_rnn(rnn_cell, self.X, sequence_length=self.seq_len, dtype=tf.float32)\n",
    "\n",
    "        last_rnn_output = outputs[:, -1]\n",
    "        last_rnn_output = tf.nn.dropout(last_rnn_output, self.dropout_keep_prob)\n",
    "\n",
    "    # network's output\n",
    "    with tf.device('/cpu:0'), tf.variable_scope(\"output\"):\n",
    "        self.output = tf.add(tf.matmul(last_rnn_output, self.w['out']), self.b['out']) # logit\n",
    "        self.y_hat = tf.argmax(self.output, 1, name='y_hat') # predicted labels\n",
    "\n",
    "    # network's losses\n",
    "    with tf.device('/cpu:0'), tf.name_scope(\"loss\"):\n",
    "        # cross-entropy loss\n",
    "        self.output_loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.output)\n",
    "        self.output_loss = tf.reduce_mean(self.output_loss) # summing over all samples of the batch\n",
    "\n",
    "        # add on regularization\n",
    "        l2_loss = self.config.l2_reg_lambda * \\\n",
    "                  sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "\n",
    "        # total loss\n",
    "        self.loss = self.output_loss + l2_loss\n",
    "\n",
    "    # calculate accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(self.y_hat, tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
